## [WebRTC](WebRTC.md)
> 왜 서비스에서 SFU 방식을 채택해서 사용했나요?

P2P 방식은 참여자가 N명일 때 각 사용자가 N - 1개의 스트림을 업로드해야 합니다. 일반적인 가정용 네트워크는 다운로드보다 업로드 대역폭이 제한적이기 때문에 참여자가 5명만 초과하더라도 네트워크 끊김 현상이 발생할 수 있습니다. 

SFU 방식을 사용하게 된다면 클라이언트는 서버로 1개의 스트림만 업로드하면 되게 되어 클라이언트 부하를 P2P 방식보다 획기적으로 줄일 수 있습니다. 저희 서비스는 미디어 업로드 외에 재스처 인식이 클라이언트에서 실행되기 때문에 클라이언트 부하를 줄이는 것이 중요했습니다.

또한, MCU는 서버가 모든 영상 혹은 오디오를 하나로 합쳐서 내려주기 때문에 클라이언트는 매우 편리하지만, 서버에서 실시간으로 수많은 영상, 오디오를 디코딩하고 다시 합성하는 과정에서 엄청난 CPU 리소스를 소모하여 지연시간이 발생하게 됩니다.

하지만, SFU 방식을 사용하게 되면 서버는 미디어 패킷을 열어보거나 가공하지 않고, 단순히 목적지로 전달하기만 하여 서버 부하가 MCU보다 훨씬 낮으며 지연 시간 또한 P2P와 거의 차이가 없을 정도로 빠르게 됩니다.

결론적으로 사용자 기기의 부하를 최소화하면서 서버의 운영 비용을 절감하고 실시간성을 유지하기 위해 SFU 방식을 채택하였습니다.

> 왜 SFU 방식을 직접 구현하지 않았나요?

WebRTC 프로토콜은 단순히 데이터를 주고받는 것을 넘어, 매우 복잡한 표준 규격들을 포함하고 있습니다. ICE(연결), DTLS(암호화), SRTP(미디어 전송), SCTP(데이터) 등 수많은 하위 프로토콜을 바닥부터 안정적으로 구현하는 데는 막대한 시간과 비용이 소모됩니다. 한정된 시간 내에 **'바퀴를 새로 만드는 것'보다 검증된 엔진을 활용해 서비스의 핵심 가치(제스처 인식, 실시간 요약)를 완성하는 것이 엔지니어로서 더 합리적인 선택**이라고 판단했습니다.

실시간 통신은 네트워크 지연이나 패킷 손실에 매우 민감합니다. 이미 전 세계적으로 수많은 서비스에서 검증된 **mediasoup의 C++ 코어**를 활용함으로써, 통신 품질의 안정성을 확보하고 저희 서비스의 핵심 로직인 **'제스처 인식 및 처리'**에 더 집중하기 위해 라이브러리 사용을 결정했습니다.

> 왜 mediasoup라는 기술을 사용하여 SFU 방식을 구현하였나요?

Node.js 환경에서 SFU 방식을 편리하게 구현할 수 있는 도구는 Mediasoup 외에도, LiveKit, Janus, Jitsi 등의 도구가 있습니다. 이 도구 중 Mediasoup를 선택하여 SFU 방식을 구현한 이유는 다음과 같습니다.

첫째로, Node.js 환경 내 통합이었습니다. 저희 서비스의 메인 백엔드 환경인 Node.js 내에서 라이브러리 형태로 직접 제어할 수 있어 별도의 서버 바이너리를 관리해야하는 Janus나 Go 기반의 LiveKit 보다 관리 포인트가 적었습니다.

두번째로는, 세밀하게 관리할 수 있다는 점이었습니다. LiveKit은 시그널링부터 인증까지 모두 정해진 규칙을 따라야 하지만, mediasoup는 미디어 엔진 역할만을 수행합니다. 그래서 미디어 스트림을 전송하면서 STT 변환, 강의 요약을 실시간으로 수행하는 저희 서비스 로직을 구현하는데 제약이 없었습니다.

마지막으로는, 높은 성능과 확장성이었습니다. mediasoup는 Node.js의 싱글 스데르 한계를 극복하기 위해 CPU 코어당 Worker 프로세스를 생성하여 병렬 처리하는 아키텍처를 가지고 있습니다. 이를 통해 대규모 접속자가 발생하더라도 서버 자원을 효율적으로 사용할 수 있었습니다.

> mediasoup 핵심 객체를 어떻게 활용했나요?

먼저 서버에서 CPU 코어 수만큼 Worker를 생성하고, 각 방(Room)에 해당하는 Router를 할당했습니다. 클라이언트가 접속하면 송수신용 Transport를 생성한 뒤 미디어를 보내는 쪽에 Producer, 받는 쪽에 Consumer를 생성하면서 양방향 스트림을 관리할 수 있도록 하였습니다. 특히 STT 처리를 위해 서버 단에서 특정 오디오 Producer의 데이터를 가로채는 로직을 구현했습니다. 

또한, 각 방에 너무 많은 사용자가 몰리는 경우를 대비하여 한 방에 Router를 동적으로 관리하여 각 Router에는 임계값 이상의 사용자의 Transport가 연결되지 않도록 설계하였습니다.

> 많은 Router와 Worker 사이에서 트래픽을 어떻게 분산했나요?

서버의 안정성을 위해 **Prometheus와 Grafana**를 연동하여 각 Worker의 CPU 사용률과 Transport 점유율을 실시간으로 모니터링했습니다.

기본적으로 새로운 강의실(Room)이 생성될 때는 **라운드 로빈(Round-Robin)** 방식으로 Worker를 배정하여 부하를 균등하게 분산시켰습니다. 하지만 단순히 순서대로 배정하는 것에 그치지 않고, 특정 Router에 사용자 연결(Transport)이 임계치에 도달하거나 Worker의 리소스가 부족해질 경우, **Multi-Router Manager**를 통해 새로운 Router를 동적으로 추가 생성하여 부하를 격리하도록 설계했습니다."
## STT 변환 파이프라인
> 전체적인 미디어 파이프라인을 요약해 주세요

1. 사용자가 오디오 장치를 서비스에 연결시켜 오디오 Producer를 생성하면
2. 서버에서 PlainTransport를 통해 오디오 스트림을 가로챕니다.
3. 이러한 오디오 스트림을 FFmpeg 프로세스를 실행하여 추출하여 일정 배치마다 독립된 STT 변환 서버인 FastAPI 서버로 전달합니다.
4. 변환된 텍스트는 redis를 이용해 저장한 뒤 강의가 종료되면 외부 LLM API를 이용해 강의요약을 진행하였습니다.

이 과정에서 UDP 패킷 손실로 인해 일부 오디오 데이터가 유실될 수 있었지만, STT 엔진의 문맥 파악과 LLM의 교정 로직을 통해 어느정도 해결할 수 있었습니다.

> STT를 실시간으로 처리할 때 병목 현상은 없었나요?

미디어 전송 자체는 mediasoup의 C++ 워커가 처리하게 두어 오버헤드를 줄였습니다. STT 처리를 위해서 PlainTransport를 활용하여 미디어 패킷을 추출한 뒤 이를 FFmpeg 프로세스로 넘겨 Node.js 서버와 독립되어 오디오를 추출하고, STT 작업은 별도의 Docker Container로 동작하는 FastAPI 서버에서 동작하게 하여 병목 현상 없이 실시간으로 처리할 수 있었습니다.

> STT 처리를 위해 PlainTransport를 썼는데, 보안이나 패킷 손실 문제는 없었나요?

PlainTransport를 생성할 때 `listenInfo`를 `127.0.0.1`로 제한하여 외부 접근을 원천 차단했습니다. 또한 FFmpeg와 FastAPI 서버가 동일한 VPC 또는 프라이빗 네트워크 내에 존재하도록 구성하여, 미디어 패킷이 외부 인터넷망을 거치지 않게 설계함으로써 보안성을 확보하여 위협이 낮다고 판단했습니다. 

 또한, UDP 기반인 WebRTC 특성상 발생할 수 있는 패킷 손실은 STT 엔진의 문맥 파악 알고리즘과 추후 LLM에서의 문맥 파악으로 어느 정도 보정할 수 있었습니다. 실시간성을 위해 약간의 손실을 감수하더라도 지연 시간을 최소화하는 방향으로 튜닝했습니다.

> 왜 하필 30초 단위인가요? 10초나 60초가 아닌 30초가 최적이라고 판단한 기술적 근거는 무엇인가요?

우리 서비스의 목표는 강의 종료 후 2~3분 내 요약본 제공이었습니다. 하지만 테스트 결과 1개 CPU 환경에서 5분이 넘어가는 음성 데이터는 STT 처리 시간이 비선형적으로 늘어나는 병목 현상이 있었습니다. 따라서 전체 데이터를 한 번에 처리하기보다 **5분(300초) 단위의 배치 처리**가 필수적이었습니다.

10초 단위로 저장했을 때는 파일 생성/삭제 이벤트와 디바이스 I/O가 너무 빈번하게 발생하여 서버 CPU 점유율이 불안정해지는 것을 확인했습니다. 60초 단위는 파일 하나당 크기가 커져서, 예기치 못한 네트워크 단절 시 손실되는 데이터의 양(최대 1분 분량)이 너무 많다고 판단했습니다. 30초는 **데이터 유실의 리스크를 최소화**하면서도, 10번의 수집으로 **STT 효율이 가장 좋은 5분 단위 배치**를 구성하기에 가장 안정적인 주기였습니다.

> 30초짜리 WAV 파일 10개를 하나로 합칠 때 발생하는 오버헤드는 어떻게 해결하셨나요?

가장 큰 오버헤드는 음성 데이터를 다시 계산하는 '디코딩/인코딩' 과정에서 발생합니다. 저는 이를 해결하기 위해 FFmpeg의 **`concat` 데이지 체인 방식**을 사용했습니다. 10개의 조각을 단순히 이어 붙일 때는 원본 스트림을 그대로 복사(`copy`)하는 방식을 취해 CPU 사용량을 낮췄습니다.

10개의 파일을 각각 명령어로 넘기지 않고, **`list.txt`라는 임시 파일**을 만들어 FFmpeg가 한 번의 I/O 프로세스로 모든 파일을 읽어 들이도록 설계했습니다. 이를 통해 파일 핸들을 반복해서 열고 닫는 시스템 콜 오버헤드를 줄였습니다.

다만, 최종 병합본을 STT 워커로 보낼 때는 `libmp3lame`을 사용하여 **MP3로 인코딩** 과정을 한 번 거쳤습니다. 이는 병합 오버헤드보다 **네트워크 전송 오버헤드**가 더 크다고 판단했기 때문입니다.

> 왜 5분이 넘어가면 STT 처리 시간이 비선형적으로 늘어났나요?

Faster-Whisper 내부적으로 음성 구간을 탐지하고 `beam_size`만큼의 후보군을 계산하는 과정이 있습니다. 오디오가 길어질수록 컨텍스트 윈도우 내에서 참조해야 할 정보가 많아지고, 특히 **CPU 환경에서는 병렬 연산의 한계**로 인해 특정 길이 이상에서 연산 큐가 쌓이며 지연 시간이 급증하는 것을 확인했습니다. 이를 해결하기 위해 30초 조각 병합 후 5분 단위 배치를 선택한 것입니다.

> **10개의 파일을 병렬로 삭제(`Promise.all`)하면 I/O 부하가 있지 않나요?

현재 코드에서는 `for...of` 문 내에서 `await unlink`를 사용하여 순차적으로 삭제하고 있습니다. 이는 I/O Peak를 방지하기 위한 선택이었지만, 만약 트래픽이 더 몰린다면 **삭제 전용 큐**를 두어 백그라운드에서 여유로울 때 처리하는 방식으로 개선할 수 있다고 생각합니다.
